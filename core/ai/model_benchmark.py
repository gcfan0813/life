"""
æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œé¢„çƒ­æ¨¡å—
"""

import time
import statistics
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    model_name: str
    model_size: str
    load_time: float
    first_token_time: float
    avg_token_time: float
    tokens_per_second: float
    memory_usage_mb: float
    test_prompt: str
    test_output_length: int
    success: bool
    error_msg: Optional[str] = None

class ModelBenchmark:
    """æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.results = []
    
    def warm_up_model(self, model_config=None, warm_up_iterations: int = 3) -> bool:
        """é¢„çƒ­æ¨¡å‹"""
        print("[Benchmark] å¼€å§‹æ¨¡å‹é¢„çƒ­...")
        
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if not self.model_manager.load_model(
                model_size=model_config.size if model_config else None,
                model_name=model_config.name if model_config else None
            ):
                print("[Benchmark] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•é¢„çƒ­")
                return False
            
            # æ‰§è¡Œå‡ æ¬¡ç©ºç”Ÿæˆæ¥é¢„çƒ­
            warm_up_prompts = [
                "ä½ å¥½",
                "1+1ç­‰äºå‡ ",
                "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"
            ]
            
            for i in range(warm_up_iterations):
                prompt = warm_up_prompts[i % len(warm_up_prompts)]
                result = self.model_manager.generate(
                    prompt=prompt,
                    max_tokens=50,
                    temperature=0.7
                )
                
                if result["success"]:
                    print(f"[Benchmark] é¢„çƒ­è¿­ä»£ {i+1}/{warm_up_iterations} å®Œæˆ")
                else:
                    print(f"[Benchmark] é¢„çƒ­è¿­ä»£ {i+1} å¤±è´¥: {result.get('error')}")
                
                time.sleep(0.5)  # çŸ­æš‚ä¼‘æ¯
            
            print("[Benchmark] æ¨¡å‹é¢„çƒ­å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"[Benchmark] é¢„çƒ­è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def benchmark_model(self, 
                       model_config=None,
                       test_prompts: Optional[List[str]] = None,
                       iterations: int = 3) -> BenchmarkResult:
        """å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"[Benchmark] å¼€å§‹åŸºå‡†æµ‹è¯•: {model_config.name if model_config else 'æ¨èæ¨¡å‹'}")
        
        result = BenchmarkResult(
            model_name=model_config.name if model_config else "unknown",
            model_size=model_config.size.value if model_config else "unknown",
            load_time=0,
            first_token_time=0,
            avg_token_time=0,
            tokens_per_second=0,
            memory_usage_mb=0,
            test_prompt="",
            test_output_length=0,
            success=False
        )
        
        try:
            # è®°å½•åŠ è½½æ—¶é—´
            load_start = time.time()
            if not self.model_manager.load_model(
                model_size=model_config.size if model_config else None,
                model_name=model_config.name if model_config else None
            ):
                result.error_msg = "æ¨¡å‹åŠ è½½å¤±è´¥"
                return result
            
            result.load_time = time.time() - load_start
            
            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            try:
                import psutil
                process = psutil.Process()
                result.memory_usage_mb = process.memory_info().rss / 1024 / 1024
            except:
                pass
            
            # é¢„çƒ­æ¨¡å‹
            self.warm_up_model(model_config, warm_up_iterations=1)
            
            # æµ‹è¯•æç¤ºè¯
            if not test_prompts:
                test_prompts = [
                    "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
                    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—",
                    "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"
                ]
            
            load_times = []
            token_times = []
            token_counts = []
            
            for i, prompt in enumerate(test_prompts[:iterations]):
                print(f"[Benchmark] æµ‹è¯• {i+1}/{min(len(test_prompts), iterations)}: {prompt[:30]}...")
                
                # æµ‹è¯•åŠ è½½æ—¶é—´ï¼ˆç¬¬äºŒæ¬¡åº”è¯¥æ›´å¿«ï¼‰
                gen_start = time.time()
                
                gen_result = self.model_manager.generate(
                    prompt=prompt,
                    max_tokens=100,
                    temperature=0.7
                )
                
                if not gen_result["success"]:
                    result.error_msg = gen_result.get("error", "ç”Ÿæˆå¤±è´¥")
                    return result
                
                total_time = time.time() - gen_start
                load_times.append(total_time)
                
                # ä¼°ç®—tokenæ•°é‡å’Œé€Ÿåº¦
                output_text = gen_result["text"]
                token_count = len(output_text.split())  # ç®€å•ä¼°ç®—
                token_counts.append(token_count)
                
                if token_count > 0:
                    token_time = total_time / token_count
                    token_times.append(token_time)
                
                # è®°å½•ç¬¬ä¸€ä¸ªæµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯
                if i == 0:
                    result.test_prompt = prompt
                    result.test_output_length = len(output_text)
            
            # è®¡ç®—ç»“æœ
            if load_times:
                result.first_token_time = load_times[0]  # ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„å®Œæ•´æ—¶é—´
                result.avg_token_time = statistics.mean(token_times) if token_times else 0
                result.tokens_per_second = 1 / result.avg_token_time if result.avg_token_time > 0 else 0
            
            result.success = True
            print(f"[Benchmark] æµ‹è¯•å®Œæˆ - åŠ è½½æ—¶é—´: {result.load_time:.2f}s, é€Ÿåº¦: {result.tokens_per_second:.1f} tokens/s")
            
        except Exception as e:
            result.error_msg = str(e)
            print(f"[Benchmark] åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        
        self.results.append(result)
        return result
    
    def benchmark_all_models(self, iterations: int = 2) -> List[BenchmarkResult]:
        """æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        print("[Benchmark] å¼€å§‹æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹...")
        
        all_results = []
        available_models = self.model_manager.get_available_models()
        
        for model_info in available_models:
            if not model_info["available"]:
                print(f"[Benchmark] è·³è¿‡ä¸å¯ç”¨æ¨¡å‹: {model_info['name']}")
                continue
            
            # æŸ¥æ‰¾å¯¹åº”çš„ModelConfig
            model_config = None
            for configs in self.model_manager.model_configs.values():
                for config in configs:
                    if config.name == model_info["name"]:
                        model_config = config
                        break
                if model_config:
                    break
            
            if model_config:
                result = self.benchmark_model(model_config, iterations=iterations)
                all_results.append(result)
        
        return all_results
    
    def get_best_model_for_speed(self) -> Optional[BenchmarkResult]:
        """è·å–é€Ÿåº¦æœ€å¿«çš„æ¨¡å‹"""
        successful_results = [r for r in self.results if r.success]
        if not successful_results:
            return None
        
        return max(successful_results, key=lambda x: x.tokens_per_second)
    
    def get_best_model_for_quality(self) -> Optional[BenchmarkResult]:
        """è·å–è´¨é‡æœ€å¥½çš„æ¨¡å‹ï¼ˆåŸºäºquality_scoreå’Œæ€§èƒ½å¹³è¡¡ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚å®šä¹‰è´¨é‡è¯„ä¼°æ ‡å‡†
        # ç›®å‰ç®€å•è¿”å›ç»¼åˆè¡¨ç°æœ€å¥½çš„
        successful_results = [r for r in self.results if r.success]
        if not successful_results:
            return None
        
        # ç»¼åˆè€ƒè™‘è´¨é‡å’Œæ€§èƒ½
        def score(result):
            quality = 70  # é»˜è®¤è´¨é‡åˆ†æ•°ï¼Œå®é™…åº”è¯¥ä»model_configè·å–
            perf_score = min(result.tokens_per_second / 20, 1.0)  # æ ‡å‡†åŒ–æ€§èƒ½åˆ†æ•°
            return quality * 0.6 + perf_score * 40
        
        return max(successful_results, key=score)
    
    def save_results(self, filepath: str = "model_benchmark_results.json"):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        try:
            results_data = [asdict(r) for r in self.results]
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            print(f"[Benchmark] ç»“æœå·²ä¿å­˜åˆ° {filepath}")
        except Exception as e:
            print(f"[Benchmark] ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.results:
            print("[Benchmark] æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        print("\n" + "="*60)
        print("æ¨¡å‹åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("="*60)
        
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        print(f"æ€»æµ‹è¯•æ•°: {len(self.results)}")
        print(f"æˆåŠŸ: {len(successful)}, å¤±è´¥: {len(failed)}")
        
        if successful:
            print("\næˆåŠŸæ¨¡å‹:")
            for result in successful:
                print(f"  â€¢ {result.model_name} ({result.model_size})")
                print(f"    åŠ è½½æ—¶é—´: {result.load_time:.2f}s")
                print(f"    ç”Ÿæˆé€Ÿåº¦: {result.tokens_per_second:.1f} tokens/s")
                print(f"    å†…å­˜ä½¿ç”¨: {result.memory_usage_mb:.1f}MB")
                print()
            
            best_speed = self.get_best_model_for_speed()
            best_quality = self.get_best_model_for_quality()
            
            if best_speed:
                print(f"ğŸ† æœ€å¿«æ¨¡å‹: {best_speed.model_name} ({best_speed.tokens_per_second:.1f} tokens/s)")
            if best_quality:
                print(f"ğŸ¯ æ¨èæ¨¡å‹: {best_quality.model_name}")
        
        if failed:
            print("\nå¤±è´¥æ¨¡å‹:")
            for result in failed:
                print(f"  â€¢ {result.model_name}: {result.error_msg}")

# å…¨å±€åŸºå‡†æµ‹è¯•å®ä¾‹
model_benchmark = ModelBenchmark(None)  # å°†åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®